Datahub:
  data_path: "/my/dataset.pkl"              # path to dataset: pickle, npz or hdf5
  data_format: pickle                       # pickle, npz or hdf5
                                            # features and targets' key mappings which map standard data field name in enerzyme to the data field name in your dataset
                                            # atomic quantities always end with "a" in standard field names
                                            # if some your name is exactly the same as the standard name, just leave the mapping value empty
  features:
    Ra: coord                               # atomic positions, shape [Nframe, Natom, 3] (required)
    Za: atom_type                           # atomic numbers or symbols (will be automatically converted to numbers), shape [Nframe, Natom] (required)
    Q:                                      # total charges, shape [Nframe]. If not provided, 0 is assumed for each frame
    N:                                      # total atom counts, shape [Nframe] 
                                            # if not provided, will be infered from Za
                                            # if provided, other atomic properties can be padded to the same length and truncated to the atom count when reading
  targets:
    E: energy                               # total energy, shape [Nframe]
    Fa: grad                                # atomic forces, shape [Nframe, Natom, 3]
    Qa: chrg                                # atomic charges, shape [Nframe, Natom]
    Q:
    M2: dipole                              # total dipole moment, shape [Nframe, 3]
  preload: false                            # load the processed data from the dump directory
  neighbor_list: full                       # neighbor list building algorithm:
                                            # full: all atom pairs are neighbors
  transforms:
    atomic_energy: "/my/atomic/energy.csv"  # path to store atomic_energy. 
                                            # summation of atomic reference energies in each frame will be deducted from the total energy
    negative_gradient: true                 # flip the sign of the gradient, use it when the Fa targets are gradients instead of forces
    total_energy_normalization: false       # normalize the total energy globally by substract the mean and divide the standard deviation
Modelhub:
  internal_FFs:                             # force fields built in enerzyme
    FF01:                                   # force field ID
      suffix:                               # descriptive suffix
      architecture: PhysNet                 # architecture of the core
      active: false                         # if used in current training
      loss:
        mae:                                # mean absolute error, key-value pairs below are targets and weights
          Qa: 1
          Fa: 52.917721
          E: 1
          M2: 1.8897261
          Q: 1
        nh_penalty:                         # special loss term for PhysNet architecture
          weight: 0.01
      Metric:                               # metric defined for this model
        Qa:                                 # metric term
          rmse: 1                           # key-value pair of metric type (rmse/mae) and weight
                                            # weighted summation of metric is the 'judge_score' for earlystopping
        Fa:
          rmse: 52.917721
        E:
          rmse: 1
        M2:
          rmse: 1.8897261
        Q:
          rmse: 1
      build_params:                         # parameters shared by the whole model
        dim_embedding: 128                  # dimension of atom embeddings
        num_rbf: 64                         # number of radial basis functions
        max_Za: 94                          # maximum atomic number
        cutoff_sr: 10.0                     # short range cutoff distance
        cutoff_lr: null                     # long range cutoff distance
        Hartree_in_E: 1                     # numerical value of one Hartree in the dataset
        Bohr_in_R: 0.5291772108             # numerical value of one Bohr in the dataset
        cutoff_fn: polynomial               # flavor of the cutoff function, polynomial / bump / smooth
      layers:
        - name: RangeSeparation             # separate short range distances from long range distances and calculate the cutoff function values
                                            # cutoff_sr, cutoff_fn in build_params
        - name: ExponentialGaussianRBF      # exponential Gaussian radial basis function
          params:                           # num_rbf, cutoff_sr, cutoff_fn in build_params
            no_basis_at_infinity: false     # if a basis "centered at infinity" is contained
            init_alpha: 1                   # initial exponential scaling factor
            exp_weighting: false            # if exponential weighted basis function is used
            learnable_shape: true           # if centers and widths of Gaussians are learnable
            init_width_flavor: PhysNet      # widths initialization flavor, PhysNet or SpookyNet
        - name: RandomAtomEmbedding         # random embedding based on the atomic number
                                            # dim_embedding, max_Za in build_params
        - name: Core                        # physNet Core
          params:                           # dim_embedding, num_rbf in build_params
            num_blocks: 5                   # number of interaction blocks and output blocks to accumulate atomic properties
            num_residual_atomic: 2          # number of residual layers in the post residual stack of the interaction block
            num_residual_interaction: 3     # number of residual layers in the interaction layer
            num_residual_output: 1          # number of residual layers in the output block
            activation_fn: shifted_softplus # activation function type: shifted_softplus or swiss
            activation_params:
              dim_feature: 1                # dimension of features passing through the activation function, 1 is compatible with any shape of features
              initial_alpha: 1              # initial scaling parameter
              initial_beta: 1               # initial temperature parameter
              learnable: false              # learnable scaling and temperature parameter
            dropout_rate: 0.0               # dropout rate of all dropout layers
        - name: AtomicAffine                # scale and shift raw atomic prediction
          params:                           # max_Za in build_params
            shifts:
              Ea:
                values: 0                   # initial shift of atomic energy
                learnable: true             # if shifts of atomic energy is learnable
              Qa:
                values: 0                   # initial shift of atomic charge
                learnable: true         
            scales:
              Ea:
                values: 1                   # initial scale of atomic charge
                learnable: true
              Qa:
                values: 1
                learnable: true
        - name: ChargeConservation          # Scale the atomic charge to make sure the summation of atomic charges is equal to the total charge
        - name: AtomicCharge2Dipole         # Calculate the dipole moment from the atomic charge
        - name: ElectrostaticEnergy         # Calculate the atomic electrostatic energy from the atomic charge
          params:                           # Hartree_in_E, Bohr_in_R, cutoff_lr in build_params
            flavor: PhysNet                 # damping flavor, PhysNet or SpookyNet
        # - name: GrimmeD3Energy
        #   params:
        #     learnable: true
        - name: EnergyReduce                # Reduce atomic energy terms to the total energy
        - name: Force                       # Calculate the force from the total energy
    FF02:
      suffix:
      architecture: SpookyNet
      active: true
      loss:
        mae:
          Qa: 1
          Fa: 91.656181
          E: 1
          M2: 3.2731016
      Metric:
        Qa:
          rmse: 1
        Fa:
          rmse: 91.656181
        E:
          rmse: 1
        M2:
          rmse: 3.2731016
      build_params:
        dim_embedding: 64
        num_rbf: 16
        max_Za: 86
        cutoff_sr: 5.291772105638412
        Hartree_in_E: 1
        Bohr_in_R: 0.5291772108
        activation_fn: swish
      layers:
        - name: RangeSeparation
          params:
            cutoff_fn: bump
        - name: ExponentialBernsteinRBF     # exponential Bernstein polynomial radial basis function
          params:
            no_basis_at_infinity: false     # if a basis "centered at infinity" is contained
            init_alpha: 0.9448630629184640  # initial exponential scaling factor
            exp_weighting: false            # if exponential weighted basis function is used
            learnable_shape: true           # if centers and widths of Gaussians are learnable
        - name: NuclearEmbedding            # embedding based on the atomic number and the atomic electron configuration
          params: 
            zero_init: false                # If the embedding is initialized with zeros
            use_electron_config: true       # If the electron configuration is used
        - name: ElectronicEmbedding         # embedding based on the electron attributes
          params:                           # dim_embedding, activation_fn in build_params
            num_residual: 1                 # number of residual layers applied to features encoding the electronic state
            attribute: charge               # electron attribute the embedding stands for, charge or spin
        - name: ElectronicEmbedding
          params:
            num_residual: 1
            attribute: spin
        - name: Core                        # SpookyNet Core
          params:                           # dim_embedding, num_rbf, activation_fn in build_params
            num_modules: 3                  # number of modules (iterations) for constructing atomic features
            num_residual_pre: 1             # number of residual blocks applied to atomic features in each module (before other transformations)
            num_residual_local_x: 1         # number of residual blocks (per module) applied to atomic features in local interaction
            num_residual_local_s: 1         # number of residual blocks (per module) applied to s-type interaction features in local interaction
            num_residual_local_p: 1         # number of residual blocks (per module) applied to p-type interaction features in local interaction
            num_residual_local_d: 1         # number of residual blocks (per module) applied to d-type interaction features in local interaction
            num_residual_local: 1           # number of residual blocks applied to gathered atomic features in local interaction
            num_residual_nonlocal_q: 1      # number of residual blocks (per module) applied to attention query in non-local interaction
            num_residual_nonlocal_k: 1      # number of residual blocks (per module) applied to attention key in non-local interaction
            num_residual_nonlocal_v: 1      # number of residual blocks (per module) applied to attention value in non-local interaction
            num_residual_post: 1            # number of residual blocks applied to atomic features in each module (after other transformations)
            num_residual_output: 1          # number of residual blocks applied to atomic features in output branches (per module)
            use_irreps: true                # If irreducible representions (spherical harmonics) are used for p- and d-type features
            dropout_rate: 0.0               # dropout rate of all dropout layers
        - name: AtomicAffine
          params:
            shifts:
              Ea:
                values: 0
                learnable: true
              Qa:
                values: 0
                learnable: true
            scales:
              Ea:
                values: 1
                learnable: true
              Qa:
                values: 1
                learnable: true
        - name: ChargeConservation
        - name: AtomicCharge2Dipole
        # - name: ZBLRepulsionEnergy
        - name: ElectrostaticEnergy
          params:
            flavor: SpookyNet
        # - name: GrimmeD4Energy
        #   params:
        #     learnable: true
        - name: EnergyReduce
        - name: Force
Trainer:
  Splitter:
    method: random                          # splitting method: random
    parts:                                  # partition names, at least training and validation are needed
      - training
      - validation
      - test
    ratios:                                 # partition ratio (float in (0,1)) or the number of datapoints (int)
                                            # order corresponds to the parts' order
      - 0.7                                  
      - 0.1                                  
      - 0.2
    preload: false                          # if saved splitting indices are loaded
    save: true                              # if splitting indices are saved
    seed: 114514                            # splitting random seed
  Monitor:                                  # monitor prints statistics of energy terms in the total energy
                                            # optional
    E_ele:                                  # energy term name: 
                                              # E_ele for ElectrostaticEnergy layer
                                              # E_disp for GrimmeD3Energy or GrimmeD4Energy layer
                                              # E_zbl for ZBLRepulsionEnergy layer
                                            # four statistics are supported:
      - mean                                # mean value
      - std                                 # standard deviation
      - min                                 # minimum value
      - max                                 # maximum value
  seed: 114514                              # trainer random seed: dataloader shuffling, pytorch random initialization, etc.
  learning_rate: 0.001                      # learning rate, scientific notations like 1e-3 are supported, default 0.001
  warmup_ratio: 0.001                       # ratio of warmup epochs in total epochs, default 0.01
                                            # where the learning rate increases linearly from 0 to the set value
  patience: 200                             # patience of earlystopping, default 50
                                            # training stops when the number of epochs that 'judge score' doesn't decreases reach the patience
  max_norm: -1                              # gradient clipping threshold of the norm, default -1 (negative value means no clipping)
  cuda: true                                # if cuda is searched and used if possible
  weight_decay: 0                           # weight decay rate of the Adam optimizer, default 0
  batch_size: 8                             # batch size of the dataloader, default 8
  max_epochs: 10000                         # maximum epochs of training if earlystopping isn't triggered, default 1000
  dtype: float32                            # pytorch data type throughout the computation: float32 (single) / float64 (double), default float32
  use_ema: true                             # if exponential moving average is used, default true
  ema_decay: 0.999                          # decay rate of exponential moving average, default 0.999
  ema_use_num_updates: true                 # if number of updates is used in exponential moving average, default true
  data_in_memory: false                     # if whole datasets are loaded into memory, default false
  amsgrad: true                             # if amsgrad is used in Adam optimizer, default true
Metric:                                     # metrics can be defined here as default metrics
                                            # if a metric configuration is defined under a specific model, it has precedence